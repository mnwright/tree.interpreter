---
title: "Calculating MDI and MDI-oob with tree.interpreter"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{MDI}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: MDI.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The R package **tree.interpreter** at its core implements the interpretation algorithm proposed by [@saabas_interpreting_2014] for popular RF packages such as **randomForest** and **ranger**. This vignette illustrates how to calculate the MDI, a.k.a Mean Decrease Impurity, and MDI-oob, a debiased MDI feature importance measure proposed by [@li_debiased_2019], with it.

I'm planning to export all functions defined in this vignette for your convenience.

## Saabas's Prediction Interpretation Algorithm

Let's start with the interpretation algorithm by [@saabas_interpreting_2014]. The idea is to decompose the prediction for a specific sample by looking at the decision rule associated with it.

Define for a tree $T$, a feature $k$, and a sample $X$, the function $f_{T, k}(X)$ to be

$$
f_{T, k}(X) = \sum_{t \in I(T): v(t)=k} \left\{ \mu_n (t^{\text{left}}) \mathbb{1} \left(X \in R_{t^{\text{left}}}\right) + \mu_n (t^{\text{right}}) \mathbb{1} \left(X \in R_{t^{\text{right}}}\right) - \mu_n (t) \mathbb{1} \left(X \in R_t\right) \right\},
$$

where $I(T)$ is inner nodes of the tree $T$, $v(t)$ is the feature on which the node $t$ is split on, $R(t)$ is the hyper-rectangle in the feature space "occupied" by the node $t$, $\mu_n(t)$ is the average response of samples falling into $R(t)$, and $\mathbb{1}$ is the indicator function. This is calculated by the function `tree.interpreter::featureContribTree`.

Intuitively, it calculates the lagged differences of the responses for the nodes on the decision path of an individual sample, groupped by the feature on which the nodes are split on. Consequently, the sum of the response of the root node and $\sum_{k} f_{T, k}(X)$ is exactly the prediction of $X$ by $T$.

In order to move from a decision tree to a forest, define for a feature $k$ and a sample $X$ the function $f_{k}(X)$ to be

$$
f_{k}(X) = \frac{1}{n_{\text{tree}}} \sum_{s=1}^{n_{\text{tree}}} f_{T_s, k}(X),
$$

where the forest is represented by an ensemble of $n_{\text{tree}}$ trees $T_1, \dots, T_{n_{\text{tree}}}$. This is sensible because (at least for regression trees) a forest makes prediction by averaging over the predictions of its trees, so all trees naturally have the same weight. It follows that the prediction of $X$ by the whole forest is exactly the sum of the average response of the root nodes in the forest and $\sum_{k} f_{k}(X)$. This is calculated by the function `tree.interpreter::featureContrib`

Later, [@saabas_random_2015] released a Python library named **treeinterpreter** on PyPI, implementing this interpretation algorithm for random forest models by the RF library **scikit-learn**. This R package effectively serves as its R counterpart.

## MDI and MDI-oob in $f_{T, k}(X)$

Recently, [@li_debiased_2019] have shown that for a tree $T$, the MDI of the feature $k$ can be written as:

$$
\frac{1}{|\mathcal{D}^{(T)}|} \sum_{i \in \mathcal{D}^{(T)}} f_{T, k}(x_i) \cdot y_i.
$$

They also propose a debiased MDI feature importance measure using out-of-bag samples, called MDI-oob

$$
\frac{1}{|\mathcal{D} \setminus \mathcal{D}^{(T)}|} \sum_{i \in \mathcal{D} \setminus \mathcal{D}^{(T)}} f_{T, k}(x_i) \cdot y_i.
$$

## Examples

Below we present two examples to demonstrate how to calculate MDI and MDI-oob with **tree.interpreter** for regression and classification trees.

```{r setup}
library(MASS)
library(ranger)
library(tree.interpreter)
```

### Regression

In the first example, we are going to build a random forest on the Boston housing data set, and calculate the MDI/MDI-oob of each feature.

```{r init.reg}
set.seed(42L)
rf <- ranger(medv ~ ., Boston, keep.inbag = TRUE, importance = 'impurity')
tidy.RF <- tidyRF(rf, Boston[, -14], Boston[, 14])
```

#### MDI

```{r mdi.reg}
MDITree.reg <- function(tidy.RF, tree, X, Y) {
  inbag.counts <- tidy.RF$inbag.counts[[tree]]
  inbag.indices <- as.logical(inbag.counts)
  X.inbag <- X[inbag.indices, ]
  Y.inbag <- Y[inbag.indices]
  ftk.y <- featureContribTree(tidy.RF, tree, X.inbag) *
    rep(Y.inbag * inbag.counts[inbag.indices], each=ncol(X.inbag))
  apply(ftk.y, 1:2, sum) / sum(inbag.counts)
}

MDI.reg <- function(tidy.RF, X, Y) {
  Reduce(`+`, lapply(1:tidy.RF$num.trees,
                     function(tree) MDITree.reg(tidy.RF, tree, X, Y))) /
    tidy.RF$num.trees
}

(Boston.MDI <- MDI.reg(tidy.RF, Boston[, -14], Boston[, 14]))
```

You can verify the equivalence

```{r equivalence.reg}
all.equal(as.vector(Boston.MDI),
          as.vector(importance(rf) / sum(rf$inbag.counts[[1]])))
```

#### MDI-oob

```{r mdi.oob.reg}
MDIoobTree.reg <- function(tidy.RF, tree, X, Y) {
  inbag.counts <- tidy.RF$inbag.counts[[tree]]
  indices.oob <- !as.logical(inbag.counts)
  X.oob <- X[indices.oob, ]
  Y.oob <- Y[indices.oob]
  ftk.y <- featureContribTree(tidy.RF, tree, X.oob) *
    rep(Y.oob, each=ncol(X.oob))
  apply(ftk.y, 1:2, sum) / sum(indices.oob)
}

MDIoob.reg <- function(tidy.RF, X, Y) {
  Reduce(`+`, lapply(1:tidy.RF$num.trees,
                     function(tree) MDIoobTree.reg(tidy.RF, tree, X, Y))) /
    tidy.RF$num.trees
}

MDIoob.reg(tidy.RF, Boston[, -14], Boston[, 14])
```

### Classification

In the second example, we are going to build a random forest on Anderson's iris data set, and calculate the MDI/MDI-oob of each feature.

```{r init.class}
set.seed(42L)
rf <- ranger(Species ~ ., iris, keep.inbag = TRUE, importance = 'impurity')
tidy.RF <- tidyRF(rf, iris[, -5], iris[, 5])
```

#### MDI

In this case, we need to employ one hot encoding.

```{r mdi.class}
MDITree.class <- function(tidy.RF, tree, X, Y) {
  inbag.counts <- tidy.RF$inbag.counts[[tree]]
  inbag.indices <- as.logical(inbag.counts)
  X.inbag <- X[inbag.indices, ]
  Y.inbag <- t(Y[inbag.indices, ])
  ftk.y <- featureContribTree(tidy.RF, tree, X.inbag) *
    rep(Y.inbag, each = ncol(X.inbag)) *
    rep(inbag.counts[inbag.indices], each = tidy.RF$num.classes * ncol(X.inbag))
  apply(ftk.y, 1:2, sum) / sum(inbag.counts)
}

MDI.class <- function(tidy.RF, X, Y) {
  # Convert response to one hot vectors
  onehot <- matrix(0, nrow=length(Y), ncol=tidy.RF$num.classes)
  indice <- matrix(c(1:length(Y), Y[1:length(Y)]), ncol = 2)
  onehot[indice] <- 1
  Y <- onehot

  Reduce(`+`, lapply(1:tidy.RF$num.trees,
                     function(tree) MDITree.class(tidy.RF, tree, X, Y))) /
    tidy.RF$num.trees
}

(iris.MDI <- rowSums(MDI.class(tidy.RF, iris[, -5], iris[, 5])))
```

You can verify the equivalence

```{r equivalence.class}
all.equal(as.vector(iris.MDI),
          as.vector(importance(rf) / sum(rf$inbag.counts[[1]])))
```

#### MDI-oob

```{r mdi.oob.class}
MDIoobTree.class <- function(tidy.RF, tree, X, Y) {
  inbag.counts <- tidy.RF$inbag.counts[[tree]]
  indices.oob <- !as.logical(inbag.counts)
  X.oob <- X[indices.oob, ]
  Y.oob <- t(Y[indices.oob, ])
  ftk.y <- featureContribTree(tidy.RF, tree, X.oob) *
    rep(Y.oob, each = ncol(X.oob))
  apply(ftk.y, 1:2, sum) / sum(indices.oob)
}

MDIoob.class <- function(tidy.RF, X, Y) {
  # Convert response to one hot vectors
  onehot <- matrix(0, nrow=length(Y), ncol=tidy.RF$num.classes)
  indice <- matrix(c(1:length(Y), Y[1:length(Y)]), ncol = 2)
  onehot[indice] <- 1
  Y <- onehot

  Reduce(`+`, lapply(1:tidy.RF$num.trees,
                     function(tree) MDIoobTree.class(tidy.RF, tree, X, Y))) /
    tidy.RF$num.trees
}

rowSums(MDIoob.class(tidy.RF, iris[, -5], iris[, 5]))
```

## References
